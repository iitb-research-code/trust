{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "087af8f6",
   "metadata": {
    "id": "Y4YlT-8B8lLN"
   },
   "source": [
    "# Targeted Selection Demo For Biomedical Datasets With Rare Classes"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7a0fc69c",
   "metadata": {},
   "source": [
    "### Imports "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "3b905244",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "MMIAA-Ua8lLR",
    "outputId": "c379d728-9870-4fca-cbca-24658e0a12ef"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/venkat/pyvenvs/trust/lib/python3.9/site-packages/tqdm/auto.py:22: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "import random\n",
    "import datetime\n",
    "import copy\n",
    "import numpy as np\n",
    "from tabulate import tabulate\n",
    "import os\n",
    "import csv\n",
    "import json\n",
    "import subprocess\n",
    "import sys\n",
    "import PIL.Image as Image\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "import torchvision\n",
    "import torchvision.models as models\n",
    "from matplotlib import pyplot as plt\n",
    "\n",
    "sys.path.append('/home/venkat/trust/')\n",
    "\n",
    "from trust.utils.models.resnet import ResNet18\n",
    "from trust.utils.models.resnet import ResNet50\n",
    "from trust.utils.custom_dataset_medmnist import load_biodataset_custom\n",
    "from torch.utils.data import Subset\n",
    "from torch.autograd import Variable\n",
    "import tqdm\n",
    "from math import floor\n",
    "from sklearn.metrics.pairwise import cosine_similarity, pairwise_distances\n",
    "from trust.strategies.smi import SMI\n",
    "from trust.strategies.random_sampling import RandomSampling\n",
    "from trust.strategies.wassal import WASSAL\n",
    "\n",
    "sys.path.append('/home/venkat/trust/')\n",
    "\n",
    "# from distl.distil.scalable_active_learning_strategies.entropy_sampling import EntropySampling\n",
    "# from distil.distil.scalable_active_learning_strategies.badge import BADGE\n",
    "\n",
    "seed=42\n",
    "torch.manual_seed(seed)\n",
    "np.random.seed(seed)\n",
    "random.seed(seed)\n",
    "from trust.utils.utils import *\n",
    "from trust.utils.viz import tsne_smi"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ec462346",
   "metadata": {},
   "source": [
    "### Helper functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "5c36c6b4",
   "metadata": {
    "id": "ClNjNvIX8lLT"
   },
   "outputs": [],
   "source": [
    "def model_eval_loss(data_loader, model, criterion):\n",
    "    total_loss = 0\n",
    "    with torch.no_grad():\n",
    "        for batch_idx, (inputs, targets) in enumerate(data_loader):\n",
    "            inputs, targets = inputs.to(device), targets.to(device, non_blocking=True)\n",
    "            outputs = model(inputs)\n",
    "            loss = criterion(outputs, targets)\n",
    "            total_loss += loss.item()\n",
    "    return total_loss\n",
    "\n",
    "def init_weights(m):\n",
    "    if isinstance(m, nn.Conv2d):\n",
    "        torch.nn.init.xavier_uniform_(m.weight)\n",
    "    elif isinstance(m, nn.Linear):\n",
    "        torch.nn.init.xavier_uniform_(m.weight)\n",
    "        m.bias.data.fill_(0.01)\n",
    "\n",
    "def weight_reset(m):\n",
    "    if isinstance(m, nn.Conv2d) or isinstance(m, nn.Linear):\n",
    "        m.reset_parameters()\n",
    "                \n",
    "def create_model(name, num_cls, device, embedding_type):\n",
    "    if name == 'ResNet18':\n",
    "        if embedding_type == \"gradients\":\n",
    "            model = ResNet18(num_cls)\n",
    "        else:\n",
    "            model = models.resnet18()\n",
    "    elif name == 'ResNet50':\n",
    "        if embedding_type == \"gradients\":\n",
    "            model = ResNet50(num_cls)\n",
    "        else:\n",
    "            model = models.resnet50()\n",
    "    elif name == 'MnistNet':\n",
    "        model = MnistNet()\n",
    "    elif name == 'ResNet164':\n",
    "        model = ResNet164(num_cls)\n",
    "    model.apply(init_weights)\n",
    "    model = model.to(device)\n",
    "    return model\n",
    "\n",
    "def loss_function():\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "    criterion_nored = nn.CrossEntropyLoss(reduction='none')\n",
    "    return criterion, criterion_nored\n",
    "\n",
    "def optimizer_with_scheduler(model, num_epochs, learning_rate, m=0.9, wd=5e-4):\n",
    "    optimizer = optim.SGD(model.parameters(), lr=learning_rate,\n",
    "                          momentum=m, weight_decay=wd)\n",
    "    scheduler = torch.optim.lr_scheduler.CosineAnnealingLR(optimizer, T_max=num_epochs)\n",
    "    return optimizer, scheduler\n",
    "\n",
    "def optimizer_without_scheduler(model, learning_rate, m=0.9, wd=5e-4):\n",
    "#     optimizer = optim.Adam(model.parameters(),weight_decay=wd)\n",
    "    optimizer = optim.SGD(model.parameters(), lr=learning_rate,\n",
    "                          momentum=m, weight_decay=wd)\n",
    "    return optimizer\n",
    "\n",
    "def generate_cumulative_timing(mod_timing):\n",
    "    tmp = 0\n",
    "    mod_cum_timing = np.zeros(len(mod_timing))\n",
    "    for i in range(len(mod_timing)):\n",
    "        tmp += mod_timing[i]\n",
    "        mod_cum_timing[i] = tmp\n",
    "    return mod_cum_timing/3600\n",
    "\n",
    "def displayTable(val_err_log, tst_err_log):\n",
    "    col1 = [str(i) for i in range(10)]\n",
    "    val_acc = [str(100-i) for i in val_err_log]\n",
    "    tst_acc = [str(100-i) for i in tst_err_log]\n",
    "    table = [col1, val_acc, tst_acc]\n",
    "    table = map(list, zip(*table))\n",
    "    print(tabulate(table, headers=['Class', 'Val Accuracy', 'Test Accuracy'], tablefmt='orgtbl'))\n",
    "\n",
    "def find_err_per_class(test_set, val_set, final_val_classifications, final_val_predictions, final_tst_classifications, \n",
    "                       final_tst_predictions, saveDir, prefix):\n",
    "    val_err_idx = list(np.where(np.array(final_val_classifications) == False)[0])\n",
    "    tst_err_idx = list(np.where(np.array(final_tst_classifications) == False)[0])\n",
    "    val_class_err_idxs = []\n",
    "    tst_err_log = []\n",
    "    val_err_log = []\n",
    "    for i in range(num_cls):\n",
    "        tst_class_idxs = list(torch.where(torch.Tensor(test_set.targets) == i)[0].cpu().numpy())\n",
    "        val_class_idxs = list(torch.where(torch.Tensor(val_set.targets.float()) == i)[0].cpu().numpy())\n",
    "        #err classifications per class\n",
    "        val_err_class_idx = set(val_err_idx).intersection(set(val_class_idxs))\n",
    "        tst_err_class_idx = set(tst_err_idx).intersection(set(tst_class_idxs))\n",
    "        if(len(val_class_idxs)>0):\n",
    "            val_error_perc = round((len(val_err_class_idx)/len(val_class_idxs))*100,2)\n",
    "        else:\n",
    "            val_error_perc = 0\n",
    "        tst_error_perc = round((len(tst_err_class_idx)/len(tst_class_idxs))*100,2)\n",
    "#         print(\"val, test error% for class \", i, \" : \", val_error_perc, tst_error_perc)\n",
    "        val_class_err_idxs.append(val_err_class_idx)\n",
    "        tst_err_log.append(tst_error_perc)\n",
    "        val_err_log.append(val_error_perc)\n",
    "    displayTable(val_err_log, tst_err_log)\n",
    "    tst_err_log.append(sum(tst_err_log)/len(tst_err_log))\n",
    "    val_err_log.append(sum(val_err_log)/len(val_err_log))\n",
    "    return tst_err_log, val_err_log, val_class_err_idxs\n",
    "\n",
    "\n",
    "def aug_train_subset(train_set, lake_set, true_lake_set, subset, lake_subset_idxs, budget, augrandom=False):\n",
    "    all_lake_idx = list(range(len(lake_set)))\n",
    "    if(not(len(subset)==budget) and augrandom):\n",
    "        print(\"Budget not filled, adding \", str(int(budget) - len(subset)), \" randomly.\")\n",
    "        remain_budget = int(budget) - len(subset)\n",
    "        remain_lake_idx = list(set(all_lake_idx) - set(subset))\n",
    "        random_subset_idx = list(np.random.choice(np.array(remain_lake_idx), size=int(remain_budget), replace=False))\n",
    "        subset += random_subset_idx\n",
    "    if str(type(true_lake_set.targets)) == \"<class 'numpy.ndarray'>\":\n",
    "        lake_ss = SubsetWithTargets(true_lake_set, subset, torch.Tensor(true_lake_set.targets.astype(np.float))[subset])\n",
    "    else:\n",
    "        lake_ss = SubsetWithTargets(true_lake_set, subset, torch.Tensor(true_lake_set.targets.float())[subset])\n",
    "    remain_lake_idx = list(set(all_lake_idx) - set(lake_subset_idxs))\n",
    "    if str(type(true_lake_set.targets)) == \"<class 'numpy.ndarray'>\":\n",
    "        remain_lake_set = SubsetWithTargets(lake_set, remain_lake_idx, torch.Tensor(lake_set.targets.astype(np.float))[remain_lake_idx])\n",
    "    else:\n",
    "        remain_lake_set = SubsetWithTargets(lake_set, remain_lake_idx, torch.Tensor(lake_set.targets.float())[remain_lake_idx])\n",
    "    if str(type(true_lake_set.targets)) == \"<class 'numpy.ndarray'>\":\n",
    "        remain_true_lake_set = SubsetWithTargets(true_lake_set, remain_lake_idx, torch.Tensor(true_lake_set.targets.astype(np.float))[remain_lake_idx])\n",
    "    else:\n",
    "        remain_true_lake_set = SubsetWithTargets(true_lake_set, remain_lake_idx, torch.Tensor(true_lake_set.targets.float())[remain_lake_idx])\n",
    "#     print(len(lake_ss),len(remain_lake_set),len(lake_set))\n",
    "    aug_train_set = torch.utils.data.ConcatDataset([train_set, lake_ss])\n",
    "    aug_trainloader = torch.utils.data.DataLoader(train_set, batch_size=10, shuffle=True, pin_memory=True)\n",
    "    return aug_train_set, remain_lake_set, remain_true_lake_set, lake_ss\n",
    "                        \n",
    "def getQuerySet(val_set, val_class_err_idxs, imb_cls_idx, miscls):\n",
    "    miscls_idx = []\n",
    "    if(miscls):\n",
    "        for i in range(len(val_class_err_idxs)):\n",
    "            if i in imb_cls_idx:\n",
    "                miscls_idx += val_class_err_idxs[i]\n",
    "        print(\"Total misclassified examples from imbalanced classes (Size of query set): \", len(miscls_idx))\n",
    "    else:\n",
    "        for i in imb_cls_idx:\n",
    "            imb_cls_samples = list(torch.where(torch.Tensor(val_set.targets.float()) == i)[0].cpu().numpy())\n",
    "            miscls_idx += imb_cls_samples\n",
    "        print(\"Total samples from imbalanced classes as targets (Size of query set): \", len(miscls_idx))\n",
    "    return Subset(val_set, miscls_idx), val_set.targets[miscls_idx]\n",
    "\n",
    "def getPerClassSel(lake_set, subset, num_cls):\n",
    "    perClsSel = []\n",
    "    if str(type(lake_set.targets)) == \"<class 'numpy.ndarray'>\":\n",
    "        subset_cls = torch.Tensor(lake_set.targets.astype(np.float))[subset]\n",
    "    else:\n",
    "        subset_cls = torch.Tensor(lake_set.targets.float())[subset]\n",
    "    for i in range(num_cls):\n",
    "        cls_subset_idx = list(torch.where(subset_cls == i)[0].cpu().numpy())\n",
    "        perClsSel.append(len(cls_subset_idx))\n",
    "    return perClsSel\n",
    "\n",
    "def print_final_results(res_dict, sel_cls_idx):\n",
    "    print(\"Gain in overall test accuracy: \", res_dict['test_acc'][1]-res_dict['test_acc'][0])\n",
    "    bf_sel_cls_acc = np.array(res_dict['all_class_acc'][0])[sel_cls_idx]\n",
    "    af_sel_cls_acc = np.array(res_dict['all_class_acc'][1])[sel_cls_idx]\n",
    "    print(\"Gain in targeted test accuracy: \", np.mean(af_sel_cls_acc-bf_sel_cls_acc))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9d02859f",
   "metadata": {},
   "source": [
    "# Data, Model & Experimental Settings\n",
    "The CIFAR-10 dataset contains 60,000 32x32 color images in 10 different classes.The 10 different classes represent airplanes, cars, birds, cats, deer, dogs, frogs, horses, ships, and trucks. There are 6,000 images of each class. The training set contains 50,000 images and test set contains 10,000 images. We will use custom_dataset() function in Trust to simulated a class imbalance scenario using the split_cfg dictionary given below. We then use a ResNet18 model as our task DNN and train it on the simulated imbalanced version of the CIFAR-10 dataset. Next we perform targeted selection using various SMI functions and compare their gain in overall accuracy as well as on the imbalanced classes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "96ce679b",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "mkbsfjml8lLX",
    "outputId": "283a4f46-1575-40a6-e915-88d2d7a4e793"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "split_cfg: {'sel_cls_idx': [0, 1], 'per_imbclass_train': {0: 100, 1: 5}, 'per_imbclass_val': {0: 0, 1: 20}, 'per_imbclass_lake': {0: 1050, 1: 50}, 'per_imbclass_test': {0: 200, 1: 200}}\n"
     ]
    }
   ],
   "source": [
    "feature = \"classimb\"\n",
    "device_id = 0\n",
    "run=\"test_run\"\n",
    "# datadir = 'data/'\n",
    "# datadir = '/data/medmnist' #contains the npz file of the data_name dataset listed below\n",
    "datadir = '/media/data3.1/gan/data-selection-by-softsubsetting/data/pneumoniamnist/' #contains the npz file of the data_name dataset listed below\n",
    "data_name = 'pneumoniamnist'\n",
    "model_name = 'ResNet18'\n",
    "learning_rate = 0.0003\n",
    "computeClassErrorLog = True\n",
    "device = \"cuda:\"+str(device_id) if torch.cuda.is_available() else \"cpu\"\n",
    "miscls = True #Set to True if only the misclassified examples from the imbalanced classes is to be used\n",
    "embedding_type = \"gradients\" #Type of the representation to use (gradients/features)\n",
    "num_cls = 2\n",
    "budget = 5\n",
    "visualize_tsne = False\n",
    "split_cfg = {\"sel_cls_idx\":[0,1], \n",
    "             \"per_imbclass_train\":{0:100,1:5}, \n",
    "             \"per_imbclass_val\":{0:0,1:20}, \n",
    "             \"per_imbclass_lake\":{0:1050,1:50},\n",
    "             \"per_imbclass_test\":{0:200,1:200}}\n",
    "print(\"split_cfg:\",split_cfg)\n",
    "initModelPath = \"./\"+data_name + \"_\" + model_name + \"_\" + str(learning_rate) + \"_\" + str(split_cfg[\"sel_cls_idx\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d6d2a173",
   "metadata": {
    "id": "9qKXzKtd8lLZ"
   },
   "source": [
    "# Targeted Selection Algorithm\n",
    "1. Given: Initial Labeled set of Examples: ùê∏, large unlabeled dataset: ùëà, A target subset/slice where we want to improve accuracy: ùëá, Loss function ùêø for learning\n",
    "2. Train model with loss $\\mathcal L$ on labeled set $E$ and obtain parameters $\\theta_E$\n",
    "3. Compute the gradients $\\{\\nabla_{\\theta_E} \\mathcal L(x_i, y_i), i \\in U\\}$ (using hypothesized labels) and $\\{\\nabla_{\\theta_E} \\mathcal L(x_i, y_i), i \\in T\\}$. \n",
    "(This notebook uses gradients for representation. However, any other representation can be used. Trust also supports using features via the API.)\n",
    "4. Compute the similarity kernels $S$ (this includes kernel of the elements within $U$, within $T$ and between $U$ and $T$) and define a submodular function $f$ and diversity function $g$\n",
    "5. Compute subset $\\hat{A}$ by mazximizing the SMI function: $\\hat{A} \\gets \\max_{A \\subseteq U, |A|\\leq k} I_f(A;T) + \\gamma g(A)$\n",
    "6. Obtain the labels of the elements in $A^*$: $L(\\hat{A})$\n",
    "7. Train a model on the combined labeled set $E \\cup L(\\hat{A})$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "91b510c9",
   "metadata": {
    "id": "mMSfzzqM8lLZ"
   },
   "outputs": [],
   "source": [
    "def run_targeted_selection(dataset_name, datadir, feature, model_name, budget, split_cfg, learning_rate, run,\n",
    "                device, computeErrorLog, strategy=\"SIM\", sf=\"\"):\n",
    "\n",
    "    #load the dataset in the class imbalance setting\n",
    "    train_set, val_set, test_set, lake_set, sel_cls_idx, num_cls = load_biodataset_custom(datadir, dataset_name, feature, split_cfg, False, False)\n",
    "    print(\"Indices of randomly selected classes for imbalance: \", sel_cls_idx)\n",
    "    \n",
    "    #Set batch size for train, validation and test datasets\n",
    "    N = len(train_set)\n",
    "    trn_batch_size = 20\n",
    "    val_batch_size = 10\n",
    "    tst_batch_size = 100\n",
    "\n",
    "    #Create dataloaders\n",
    "    trainloader = torch.utils.data.DataLoader(train_set, batch_size=trn_batch_size,\n",
    "                                              shuffle=True, pin_memory=True)\n",
    "\n",
    "    valloader = torch.utils.data.DataLoader(val_set, batch_size=val_batch_size, \n",
    "                                            shuffle=False, pin_memory=True)\n",
    "\n",
    "    tstloader = torch.utils.data.DataLoader(test_set, batch_size=tst_batch_size,\n",
    "                                             shuffle=False, pin_memory=True)\n",
    "    \n",
    "    lakeloader = torch.utils.data.DataLoader(lake_set, batch_size=tst_batch_size,\n",
    "                                         shuffle=False, pin_memory=True)\n",
    "    true_lake_set = copy.deepcopy(lake_set)\n",
    "    # Budget for subset selection\n",
    "    bud = budget\n",
    "   \n",
    "    # Variables to store accuracies\n",
    "    num_rounds=2 #The first round is for training the initial model and the second round is to train the final model\n",
    "    fulltrn_losses = np.zeros(num_rounds)\n",
    "    val_losses = np.zeros(num_rounds)\n",
    "    tst_losses = np.zeros(num_rounds)\n",
    "    timing = np.zeros(num_rounds)\n",
    "    val_acc = np.zeros(num_rounds)\n",
    "    full_trn_acc = np.zeros(num_rounds)\n",
    "    tst_acc = np.zeros(num_rounds)\n",
    "    final_tst_predictions = []\n",
    "    final_tst_classifications = []\n",
    "    best_val_acc = -1\n",
    "    csvlog = []\n",
    "    val_csvlog = []\n",
    "    # Results logging file\n",
    "    all_logs_dir = './results/' + dataset_name  + '/' + feature + '/'+  sf + '/' + str(bud) + '/' + str(run)\n",
    "#     print(\"Saving results to: \", all_logs_dir)\n",
    "#     subprocess.run([\"mkdir\", \"-p\", all_logs_dir]) #Uncomment for saving results\n",
    "    exp_name = dataset_name + \"_\" + feature +  \"_\" + strategy + \"_\" + str(len(sel_cls_idx))  +\"_\" + sf +  '_budget:' + str(bud) + '_rounds:' + str(num_rounds) + '_runs' + str(run)\n",
    "\n",
    "    #Create a dictionary for storing results and the experimental setting\n",
    "    res_dict = {\"dataset\":data_name, \n",
    "                \"feature\":feature, \n",
    "                \"sel_func\":sf,\n",
    "                \"sel_budget\":budget, \n",
    "                \"num_selections\":num_rounds-1, \n",
    "                \"model\":model_name, \n",
    "                \"learning_rate\":learning_rate, \n",
    "                \"setting\":split_cfg, \n",
    "                \"all_class_acc\":None, \n",
    "                \"test_acc\":[],\n",
    "                \"sel_per_cls\":[], \n",
    "                \"sel_cls_idx\":sel_cls_idx}\n",
    "    \n",
    "    # Model Creation\n",
    "    model = create_model(model_name, num_cls, device, embedding_type)\n",
    "    model1 = create_model(model_name, num_cls, device, embedding_type)\n",
    "    strategy_args = {'batch_size': 20, 'device':device, 'embedding_type':'gradients', 'keep_embedding':True,\n",
    "                    'wd_num_epochs':50,'wd_lr':0.02,'verbose':True}\n",
    "    unlabeled_lake_set = LabeledToUnlabeledDataset(lake_set)\n",
    "    \n",
    "    if(strategy == \"AL\"):\n",
    "        if(sf==\"badge\"):\n",
    "            strategy_sel = BADGE(train_set, unlabeled_lake_set, model, num_cls, strategy_args)\n",
    "        elif(sf==\"us\"):\n",
    "            strategy_sel = EntropySampling(train_set, unlabeled_lake_set, model, num_cls, strategy_args)\n",
    "        elif(sf==\"glister\" or sf==\"glister-tss\"):\n",
    "            strategy_sel = GLISTER(train_set, unlabeled_lake_set, model, num_cls, strategy_args, val_set, typeOf='rand', lam=0.1)\n",
    "        elif(sf==\"gradmatch-tss\"):\n",
    "            strategy_sel = GradMatchActive(train_set, unlabeled_lake_set, model, num_cls, strategy_args, val_set)\n",
    "        elif(sf==\"coreset\"):\n",
    "            strategy_sel = CoreSet(train_set, unlabeled_lake_set, model, num_cls, strategy_args)\n",
    "        elif(sf==\"leastconf\"):\n",
    "            strategy_sel = LeastConfidence(train_set, unlabeled_lake_set, model, num_cls, strategy_args)\n",
    "        elif(sf==\"margin\"):\n",
    "            strategy_sel = MarginSampling(train_set, unlabeled_lake_set, model, num_cls, strategy_args)\n",
    "    if(strategy == \"SIM\"):\n",
    "        strategy_args['smi_function'] = sf\n",
    "        strategy_sel = SMI(train_set, unlabeled_lake_set, val_set, model, num_cls, strategy_args)\n",
    "    if(strategy == \"random\"):\n",
    "        strategy_sel = RandomSampling(train_set, unlabeled_lake_set, model, num_cls, strategy_args)\n",
    "    if(strategy == \"WASSAL\"):\n",
    "        strategy_sel = WASSAL(train_set, unlabeled_lake_set, val_set, model, num_cls, strategy_args)\n",
    "        \n",
    "    # Loss Functions\n",
    "    criterion, criterion_nored = loss_function()\n",
    "\n",
    "    # Getting the optimizer and scheduler\n",
    "    optimizer = optimizer_without_scheduler(model, learning_rate)\n",
    "\n",
    "    for i in range(num_rounds):\n",
    "        tst_loss = 0\n",
    "        tst_correct = 0\n",
    "        tst_total = 0\n",
    "        val_loss = 0\n",
    "        val_correct = 0\n",
    "        val_total = 0\n",
    "        \n",
    "        if(i==0):\n",
    "            print(\"Initial training epoch\")\n",
    "            if(os.path.exists(initModelPath)): #Read the initial trained model if it exists\n",
    "                model.load_state_dict(torch.load(initModelPath, map_location=device))\n",
    "                print(\"Init model loaded from disk, skipping init training: \", initModelPath)\n",
    "                model.eval()\n",
    "                with torch.no_grad():\n",
    "                    final_val_predictions = []\n",
    "                    final_val_classifications = []\n",
    "                    for batch_idx, (inputs, targets) in enumerate(valloader):\n",
    "                        inputs, targets = inputs.to(device), targets.to(device, non_blocking=True)\n",
    "                        outputs = model(inputs)\n",
    "                        loss = criterion(outputs, targets)\n",
    "                        val_loss += loss.item()\n",
    "                        _, predicted = outputs.max(1)\n",
    "                        val_total += targets.size(0)\n",
    "                        val_correct += predicted.eq(targets).sum().item()\n",
    "                        final_val_predictions += list(predicted.cpu().numpy())\n",
    "                        final_val_classifications += list(predicted.eq(targets).cpu().numpy())\n",
    "  \n",
    "                    final_tst_predictions = []\n",
    "                    final_tst_classifications = []\n",
    "                    for batch_idx, (inputs, targets) in enumerate(tstloader):\n",
    "                        inputs, targets = inputs.to(device), targets.to(device, non_blocking=True)\n",
    "                        outputs = model(inputs)\n",
    "                        loss = criterion(outputs, targets)\n",
    "                        tst_loss += loss.item()\n",
    "                        _, predicted = outputs.max(1)\n",
    "                        tst_total += targets.size(0)\n",
    "                        tst_correct += predicted.eq(targets).sum().item()\n",
    "                        final_tst_predictions += list(predicted.cpu().numpy())\n",
    "                        final_tst_classifications += list(predicted.eq(targets).cpu().numpy())                \n",
    "                    best_val_acc = (val_correct/val_total)\n",
    "                    val_acc[i] = val_correct / val_total\n",
    "                    tst_acc[i] = tst_correct / tst_total\n",
    "                    val_losses[i] = val_loss\n",
    "                    tst_losses[i] = tst_loss\n",
    "                    res_dict[\"test_acc\"].append(tst_acc[i]*100)\n",
    "                continue\n",
    "        else:\n",
    "            #Remove true labels from the unlabeled dataset, the hypothesized labels are computed when select is called\n",
    "            unlabeled_lake_set = LabeledToUnlabeledDataset(lake_set)\n",
    "            strategy_sel.update_data(train_set, unlabeled_lake_set)\n",
    "            #compute the error log before every selection\n",
    "            if(computeErrorLog):\n",
    "                tst_err_log, val_err_log, val_class_err_idxs = find_err_per_class(test_set, val_set, final_val_classifications, final_val_predictions, final_tst_classifications, final_tst_predictions, all_logs_dir, sf+\"_\"+str(bud))\n",
    "                csvlog.append([100-x for x in tst_err_log])\n",
    "                val_csvlog.append([100-x for x in val_err_log])\n",
    "            ####SIM####\n",
    "            if(strategy=='WASSAL'):\n",
    "                if(feature==\"classimb\"):\n",
    "                        #make a dataloader for the misclassifications - only for experiments with targets\n",
    "                        miscls_set, miscls_set_targets = getQuerySet(val_set, val_class_err_idxs, sel_cls_idx, miscls)\n",
    "                        strategy_sel.update_queries(miscls_set)\n",
    "\n",
    "            if(strategy==\"SIM\" or strategy==\"SF\"):\n",
    "                if(sf.endswith(\"mi\")):\n",
    "                    if(feature==\"classimb\"):\n",
    "                        #make a dataloader for the misclassifications - only for experiments with targets\n",
    "                        miscls_set, miscls_set_targets = getQuerySet(val_set, val_class_err_idxs, sel_cls_idx, miscls)\n",
    "                        strategy_sel.update_queries(miscls_set)\n",
    "            elif(strategy==\"AL\"):\n",
    "                if(sf==\"glister-tss\" or sf==\"gradmatch-tss\"):\n",
    "                    miscls_set = getQuerySet(val_set, val_class_err_idxs, sel_cls_idx, miscls)\n",
    "                    strategy_sel.update_queries(miscls_set)\n",
    "                    print(\"reinit AL with targeted miscls samples\")\n",
    "            \n",
    "            strategy_sel.update_model(model)\n",
    "            subset = strategy_sel.select(budget)\n",
    "            print(\"#### Selection Complete, Now re-training with augmented subset ####\")\n",
    "            if(visualize_tsne):\n",
    "                tsne_plt = tsne_smi(strategy_sel.unlabeled_data_embedding.cpu(),\n",
    "                                    lake_set.targets,\n",
    "                                    strategy_sel.query_embedding.cpu(),\n",
    "                                    miscls_set_targets,\n",
    "                                    subset)\n",
    "                print(\"Computed TSNE plot of the selection\")\n",
    "            lake_subset_idxs = subset #indices wrt to lake that need to be removed from the lake\n",
    "            perClsSel = getPerClassSel(true_lake_set, lake_subset_idxs, num_cls)\n",
    "            res_dict['sel_per_cls'].append(perClsSel)\n",
    "            \n",
    "            #augment the train_set with selected indices from the lake\n",
    "            train_set, lake_set, true_lake_set, add_val_set = aug_train_subset(train_set, lake_set, true_lake_set, subset, lake_subset_idxs, budget, False) #aug train with random if budget is not filled\n",
    "            print(\"After augmentation, size of train_set: \", len(train_set), \" unlabeled set: \", len(lake_set), \" val set: \", len(val_set))\n",
    "    \n",
    "#           Reinit train and lake loaders with new splits and reinit the model\n",
    "            trainloader = torch.utils.data.DataLoader(train_set, batch_size=trn_batch_size, shuffle=True, pin_memory=True)\n",
    "            lakeloader = torch.utils.data.DataLoader(lake_set, batch_size=tst_batch_size, shuffle=False, pin_memory=True)\n",
    "            model = create_model(model_name, num_cls, device, strategy_args['embedding_type'])\n",
    "            optimizer = optimizer_without_scheduler(model, learning_rate)\n",
    "                \n",
    "        #Start training\n",
    "        start_time = time.time()\n",
    "        num_ep=1\n",
    "#         while(num_ep<150):\n",
    "        while(full_trn_acc[i]<0.99 and num_ep<100):\n",
    "            model.train()\n",
    "            for batch_idx, (inputs, targets) in enumerate(trainloader):\n",
    "                inputs, targets = inputs.to(device), targets.to(device, non_blocking=True)\n",
    "                # Variables in Pytorch are differentiable.\n",
    "                inputs, target = Variable(inputs), Variable(inputs)\n",
    "                # This will zero out the gradients for this batch.\n",
    "                optimizer.zero_grad()\n",
    "                outputs = model(inputs)\n",
    "                loss = criterion(outputs, targets)\n",
    "                loss.backward()\n",
    "                optimizer.step()\n",
    "#             scheduler.step()\n",
    "          \n",
    "            full_trn_loss = 0\n",
    "            full_trn_correct = 0\n",
    "            full_trn_total = 0\n",
    "            model.eval()\n",
    "            with torch.no_grad():\n",
    "                for batch_idx, (inputs, targets) in enumerate(trainloader): #Compute Train accuracy\n",
    "                    inputs, targets = inputs.to(device), targets.to(device, non_blocking=True)\n",
    "                    outputs = model(inputs)\n",
    "                    loss = criterion(outputs, targets)\n",
    "                    full_trn_loss += loss.item()\n",
    "                    _, predicted = outputs.max(1)\n",
    "                    full_trn_total += targets.size(0)\n",
    "                    full_trn_correct += predicted.eq(targets).sum().item()\n",
    "                full_trn_acc[i] = full_trn_correct / full_trn_total\n",
    "                print(\"Selection Epoch \", i, \" Training epoch [\" , num_ep, \"]\" , \" Training Acc: \", full_trn_acc[i], end=\"\\r\")\n",
    "                num_ep+=1\n",
    "            timing[i] = time.time() - start_time\n",
    "        with torch.no_grad():\n",
    "            final_val_predictions = []\n",
    "            final_val_classifications = []\n",
    "            for batch_idx, (inputs, targets) in enumerate(valloader): #Compute Val accuracy\n",
    "                inputs, targets = inputs.to(device), targets.to(device, non_blocking=True)\n",
    "                outputs = model(inputs)\n",
    "                loss = criterion(outputs, targets)\n",
    "                val_loss += loss.item()\n",
    "                _, predicted = outputs.max(1)\n",
    "                val_total += targets.size(0)\n",
    "                val_correct += predicted.eq(targets).sum().item()\n",
    "                final_val_predictions += list(predicted.cpu().numpy())\n",
    "                final_val_classifications += list(predicted.eq(targets).cpu().numpy())\n",
    "\n",
    "            final_tst_predictions = []\n",
    "            final_tst_classifications = []\n",
    "            for batch_idx, (inputs, targets) in enumerate(tstloader): #Compute test accuracy\n",
    "                inputs, targets = inputs.to(device), targets.to(device, non_blocking=True)\n",
    "                outputs = model(inputs)\n",
    "                loss = criterion(outputs, targets)\n",
    "                tst_loss += loss.item()\n",
    "                _, predicted = outputs.max(1)\n",
    "                tst_total += targets.size(0)\n",
    "                tst_correct += predicted.eq(targets).sum().item()\n",
    "                final_tst_predictions += list(predicted.cpu().numpy())\n",
    "                final_tst_classifications += list(predicted.eq(targets).cpu().numpy())                \n",
    "            val_acc[i] = val_correct / val_total\n",
    "            tst_acc[i] = tst_correct / tst_total\n",
    "            val_losses[i] = val_loss\n",
    "            fulltrn_losses[i] = full_trn_loss\n",
    "            tst_losses[i] = tst_loss\n",
    "            full_val_acc = list(np.array(val_acc))\n",
    "            full_timing = list(np.array(timing))\n",
    "            res_dict[\"test_acc\"].append(tst_acc[i]*100)\n",
    "            print('Epoch:', i + 1, 'FullTrn,TrainAcc,ValLoss,ValAcc,TstLoss,TstAcc,Time:', full_trn_loss, full_trn_acc[i], val_loss, val_acc[i], tst_loss, tst_acc[i], timing[i])\n",
    "            print(\"Gain in accuracy: \",res_dict['test_acc'][i]-res_dict['test_acc'][i-1])\n",
    "        if(i==0): \n",
    "            print(\"Saving initial model\") \n",
    "            torch.save(model.state_dict(), initModelPath) #save initial train model if not present\n",
    "            \n",
    "    #Compute the statistics of the final model\n",
    "    if(computeErrorLog):\n",
    "        print(\"**** Final Metrics after Targeted Learning ****\")\n",
    "        tst_err_log, val_err_log, val_class_err_idxs = find_err_per_class(test_set, val_set, final_val_classifications, final_val_predictions, final_tst_classifications, final_tst_predictions, all_logs_dir, sf+\"_\"+str(bud))\n",
    "        csvlog.append([100-x for x in tst_err_log])\n",
    "        val_csvlog.append([100-x for x in val_err_log])\n",
    "        res_dict[\"all_class_acc\"] = csvlog\n",
    "        res_dict[\"all_val_class_acc\"] = val_csvlog\n",
    "        \n",
    "    #Print overall acc improvement and rare class acc improvement, show that TL selected relevant points in space, is possible show some images\n",
    "    print_final_results(res_dict, sel_cls_idx)\n",
    "    print(\"Total gain in accuracy: \",res_dict['test_acc'][i]-res_dict['test_acc'][0])\n",
    "    \n",
    "#     tsne_plt.show()\n",
    "    "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "107d9511",
   "metadata": {},
   "source": [
    "# WASSAL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "556d4da2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "pneumoniamnist Custom dataset stats: Train size:  105 Val size:  20 Lake size:  1100\n",
      "Indices of randomly selected classes for imbalance:  [0, 1]\n",
      "Initial training epoch\n",
      "Init model loaded from disk, skipping init training:  ./pneumoniamnist_ResNet18_0.0003_[0, 1]\n",
      "|   Class |   Val Accuracy |   Test Accuracy |\n",
      "|---------+----------------+-----------------|\n",
      "|       0 |            100 |           99.57 |\n",
      "|       1 |             15 |           26.41 |\n",
      "Total misclassified examples from imbalanced classes (Size of query set):  17\n",
      "There are 1100 Unlabeled dataset\n",
      "Wassertein loss,  41.42780685424805\n",
      "Wassertein loss,  32.909278869628906\n",
      "Wassertein loss,  29.46613311767578\n",
      "Wassertein loss,  28.529212951660156\n",
      "Wassertein loss,  27.144983291625977\n",
      "Wassertein loss,  26.976598739624023\n",
      "Wassertein loss,  25.309497833251953\n",
      "Wassertein loss,  23.437637329101562\n",
      "Wassertein loss,  23.009849548339844\n",
      "Wassertein loss,  22.778976440429688\n",
      "Wassertein loss,  22.713638305664062\n",
      "Wassertein loss,  22.55512237548828\n",
      "Wassertein loss,  22.11048126220703\n",
      "Wassertein loss,  22.17919921875\n",
      "Wassertein loss,  22.019357681274414\n",
      "Wassertein loss,  22.010623931884766\n",
      "Wassertein loss,  21.703645706176758\n",
      "Wassertein loss,  21.65367889404297\n",
      "Wassertein loss,  21.278728485107422\n",
      "Wassertein loss,  21.1248836517334\n",
      "Wassertein loss,  21.297014236450195\n",
      "Wassertein loss,  21.226547241210938\n",
      "Wassertein loss,  20.99261474609375\n",
      "Wassertein loss,  21.018369674682617\n",
      "Wassertein loss,  21.117530822753906\n",
      "Wassertein loss,  21.164714813232422\n",
      "Wassertein loss,  21.366226196289062\n",
      "Wassertein loss,  21.410369873046875\n",
      "Wassertein loss,  21.154193878173828\n",
      "Wassertein loss,  21.43313217163086\n",
      "Wassertein loss,  21.66909408569336\n",
      "Wassertein loss,  21.51251220703125\n",
      "Wassertein loss,  21.452892303466797\n",
      "Wassertein loss,  21.573318481445312\n",
      "Wassertein loss,  21.454242706298828\n",
      "Wassertein loss,  21.529949188232422\n",
      "Wassertein loss,  21.65044403076172\n",
      "Wassertein loss,  21.48278045654297\n",
      "Wassertein loss,  21.582050323486328\n",
      "Wassertein loss,  21.61407470703125\n",
      "Wassertein loss,  21.25987434387207\n",
      "Wassertein loss,  21.4974308013916\n",
      "Wassertein loss,  21.602516174316406\n",
      "Wassertein loss,  21.349140167236328\n",
      "Wassertein loss,  21.36083984375\n",
      "Wassertein loss,  21.405744552612305\n",
      "Wassertein loss,  21.303436279296875\n",
      "Wassertein loss,  21.412578582763672\n",
      "Wassertein loss,  21.507583618164062\n",
      "Wassertein loss,  21.82180404663086\n",
      "length of unlabelled dataset 1100\n",
      "Totals Probability of the budget: tensor(0.3716, device='cuda:0', grad_fn=<SumBackward0>)\n",
      "selected indices len  5\n",
      "#### Selection Complete, Now re-training with augmented subset ####\n",
      "After augmentation, size of train_set:  110  unlabeled set:  1095  val set:  20\n",
      "Epoch: 2 FullTrn,TrainAcc,ValLoss,ValAcc,TstLoss,TstAcc,Time: 0.3883766196668148 0.990909090909091 2.7233028411865234 0.15 7.586866676807404 0.421474358974359 2.981135845184326\n",
      "Gain in accuracy:  -11.698717948717949\n",
      "**** Final Metrics after Targeted Learning ****\n",
      "|   Class |   Val Accuracy |   Test Accuracy |\n",
      "|---------+----------------+-----------------|\n",
      "|       0 |            100 |           99.15 |\n",
      "|       1 |             15 |            7.95 |\n",
      "Gain in overall test accuracy:  -11.698717948717949\n",
      "Gain in targeted test accuracy:  -9.43999999999999\n",
      "Total gain in accuracy:  -11.698717948717949\n"
     ]
    }
   ],
   "source": [
    "run_targeted_selection(data_name,\n",
    "               datadir, \n",
    "               feature, \n",
    "               model_name, \n",
    "               budget, \n",
    "               split_cfg, \n",
    "               learning_rate, \n",
    "               run, \n",
    "               device, \n",
    "               computeClassErrorLog,\n",
    "               \"WASSAL\",'')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3240bc93",
   "metadata": {},
   "source": [
    "# Submodular Mutual Information (SMI)\n",
    "\n",
    "We let $V$ denote the ground-set of $n$ data points $V = \\{1, 2, 3,...,n \\}$ and a set function $f:\n",
    " 2^{V} \\xrightarrow{} \\Re$. Given a set of items $A, B \\subseteq V$, the submodular mutual information (MI)[1,3] is defined as $I_f(A; B) = f(A) + f(B) - f(A \\cup B)$. Intuitively, this measures the similarity between $B$ and $A$ and we refer to $B$ as the query set.\n",
    "\n",
    "In [2], they extend MI to handle the case when the target can come from an auxiliary set $V^{\\prime}$ different from the ground set $V$. For targeted data subset selection, $V$ is the source set of data instances and the target is a subset of data points (validation set or the specific set of examples of interest).\n",
    "Let $\\Omega  = V \\cup V^{\\prime}$. We define a set function $f: 2^{\\Omega} \\rightarrow \\Re$. Although $f$ is defined on $\\Omega$, the discrete optimization problem will only be defined on subsets $A \\subseteq V$. To find an optimal subset given a query set $Q \\subseteq V^{\\prime}$, we can define $g_{Q}(A) = I_f(A; Q)$, $A \\subseteq V$ and maximize the same."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5c3045e5",
   "metadata": {
    "id": "PeujlNrf8lLa"
   },
   "source": [
    "# FL1MI\n",
    "\n",
    "In the first variant of FL, we set the unlabeled dataset to be $V$. The SMI instantiation of FL1MI can be defined as:\n",
    "\\begin{align}\n",
    "I_f(A;Q)=\\sum_{i \\in V}\\min(\\max_{j \\in A}s_{ij}, \\eta \\max_{j \\in Q}sq_{ij})\n",
    "\\end{align}\n",
    "\n",
    "The first term in the min(.) of FL1MI models diversity, and the second term models query relevance. An increase in the value of $\\eta$ causes the resulting summary to become more relevant to the query."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "5a96a686",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 491,
     "referenced_widgets": [
      "d978b42c8c2d4ce69ca7a3233094ba73",
      "14a33535616d4fa993d20e525666af04",
      "bb6c0909711948b98f0a0b767cb9201e",
      "6792820b58df4d2b86386c29aaad1a9f",
      "9d386f74f6c4469bbbf1ac6b2a49afed",
      "4ee6a0573c6845458996a59679ac6068",
      "0074c4d4c28d4dad87a3eaaac8458479",
      "cbe081bb90d04faaae98fef1c9921180"
     ]
    },
    "id": "QHn0QzY78lLa",
    "outputId": "a203ea99-fb5b-4bfa-99e5-699ef0715d16",
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "pneumoniamnist Custom dataset stats: Train size:  105 Val size:  40 Lake size:  1100\n",
      "Indices of randomly selected classes for imbalance:  [0, 1]\n",
      "Initial training epoch\n",
      "Init model loaded from disk, skipping init training:  ./pneumoniamnist_ResNet18_0.0003_[0, 1]\n",
      "|   Class |   Val Accuracy |   Test Accuracy |\n",
      "|---------+----------------+-----------------|\n",
      "|       0 |            100 |           99.57 |\n",
      "|       1 |             30 |           26.41 |\n",
      "Total misclassified examples from imbalanced classes (Size of query set):  14\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[||||||||||||||||||||]100% [Iteration 25 of 25]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "#### Selection Complete, Now re-training with augmented subset ####\n",
      "After augmentation, size of train_set:  130  unlabeled set:  1075  val set:  40\n",
      "Selection Epoch  1  Training epoch [ 7 ]  Training Acc:  0.60769230769230767\r"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[20], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m run_targeted_selection(data_name, \n\u001b[1;32m      2\u001b[0m                datadir, \n\u001b[1;32m      3\u001b[0m                feature, \n\u001b[1;32m      4\u001b[0m                model_name, \n\u001b[1;32m      5\u001b[0m                budget, \n\u001b[1;32m      6\u001b[0m                split_cfg, \n\u001b[1;32m      7\u001b[0m                learning_rate, \n\u001b[1;32m      8\u001b[0m                run, \n\u001b[1;32m      9\u001b[0m                device, \n\u001b[1;32m     10\u001b[0m                computeClassErrorLog,\n\u001b[1;32m     11\u001b[0m                \u001b[39m\"\u001b[39;49m\u001b[39mSIM\u001b[39;49m\u001b[39m\"\u001b[39;49m,\u001b[39m'\u001b[39;49m\u001b[39mfl1mi\u001b[39;49m\u001b[39m'\u001b[39;49m)\n",
      "Cell \u001b[0;32mIn[18], line 207\u001b[0m, in \u001b[0;36mrun_targeted_selection\u001b[0;34m(dataset_name, datadir, feature, model_name, budget, split_cfg, learning_rate, run, device, computeErrorLog, strategy, sf)\u001b[0m\n\u001b[1;32m    205\u001b[0m                 outputs \u001b[39m=\u001b[39m model(inputs)\n\u001b[1;32m    206\u001b[0m                 loss \u001b[39m=\u001b[39m criterion(outputs, targets)\n\u001b[0;32m--> 207\u001b[0m                 loss\u001b[39m.\u001b[39;49mbackward()\n\u001b[1;32m    208\u001b[0m                 optimizer\u001b[39m.\u001b[39mstep()\n\u001b[1;32m    209\u001b[0m \u001b[39m#             scheduler.step()\u001b[39;00m\n",
      "File \u001b[0;32m~/pyvenvs/trust/lib/python3.9/site-packages/torch/_tensor.py:488\u001b[0m, in \u001b[0;36mTensor.backward\u001b[0;34m(self, gradient, retain_graph, create_graph, inputs)\u001b[0m\n\u001b[1;32m    478\u001b[0m \u001b[39mif\u001b[39;00m has_torch_function_unary(\u001b[39mself\u001b[39m):\n\u001b[1;32m    479\u001b[0m     \u001b[39mreturn\u001b[39;00m handle_torch_function(\n\u001b[1;32m    480\u001b[0m         Tensor\u001b[39m.\u001b[39mbackward,\n\u001b[1;32m    481\u001b[0m         (\u001b[39mself\u001b[39m,),\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    486\u001b[0m         inputs\u001b[39m=\u001b[39minputs,\n\u001b[1;32m    487\u001b[0m     )\n\u001b[0;32m--> 488\u001b[0m torch\u001b[39m.\u001b[39;49mautograd\u001b[39m.\u001b[39;49mbackward(\n\u001b[1;32m    489\u001b[0m     \u001b[39mself\u001b[39;49m, gradient, retain_graph, create_graph, inputs\u001b[39m=\u001b[39;49minputs\n\u001b[1;32m    490\u001b[0m )\n",
      "File \u001b[0;32m~/pyvenvs/trust/lib/python3.9/site-packages/torch/autograd/__init__.py:197\u001b[0m, in \u001b[0;36mbackward\u001b[0;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[0m\n\u001b[1;32m    192\u001b[0m     retain_graph \u001b[39m=\u001b[39m create_graph\n\u001b[1;32m    194\u001b[0m \u001b[39m# The reason we repeat same the comment below is that\u001b[39;00m\n\u001b[1;32m    195\u001b[0m \u001b[39m# some Python versions print out the first line of a multi-line function\u001b[39;00m\n\u001b[1;32m    196\u001b[0m \u001b[39m# calls in the traceback and some print out the last line\u001b[39;00m\n\u001b[0;32m--> 197\u001b[0m Variable\u001b[39m.\u001b[39;49m_execution_engine\u001b[39m.\u001b[39;49mrun_backward(  \u001b[39m# Calls into the C++ engine to run the backward pass\u001b[39;49;00m\n\u001b[1;32m    198\u001b[0m     tensors, grad_tensors_, retain_graph, create_graph, inputs,\n\u001b[1;32m    199\u001b[0m     allow_unreachable\u001b[39m=\u001b[39;49m\u001b[39mTrue\u001b[39;49;00m, accumulate_grad\u001b[39m=\u001b[39;49m\u001b[39mTrue\u001b[39;49;00m)\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "run_targeted_selection(data_name, \n",
    "               datadir, \n",
    "               feature, \n",
    "               model_name, \n",
    "               budget, \n",
    "               split_cfg, \n",
    "               learning_rate, \n",
    "               run, \n",
    "               device, \n",
    "               computeClassErrorLog,\n",
    "               \"SIM\",'fl1mi')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8b1fc84e",
   "metadata": {
    "id": "kp9_ZU7I8lLa"
   },
   "source": [
    "# FL2MI\n",
    "\n",
    "In the V2 variant, we set $D$ to be $V \\cup Q$. The SMI instantiation of FL2MI can be defined as:\n",
    "\\begin{align} \\label{eq:FL2MI}\n",
    "I_f(A;Q)=\\sum_{i \\in Q} \\max_{j \\in A} sq_{ij} + \\eta\\sum_{i \\in A} \\max_{j \\in Q} sq_{ij}\n",
    "\\end{align}\n",
    "FL2MI is very intuitive for query relevance as well. It measures the representation of data points that are the most relevant to the query set and vice versa. It can also be thought of as a bidirectional representation score."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "b1e29177",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "GZKsblhs8lLa",
    "outputId": "5983fdfb-6541-43e9-a0f0-304ddfa543b6",
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "pneumoniamnist Custom dataset stats: Train size:  105 Val size:  20 Lake size:  1100\n",
      "Indices of randomly selected classes for imbalance:  [0, 1]\n",
      "Initial training epoch\n",
      "Init model loaded from disk, skipping init training:  ./pneumoniamnist_ResNet18_0.0003_[0, 1]\n",
      "|   Class |   Val Accuracy |   Test Accuracy |\n",
      "|---------+----------------+-----------------|\n",
      "|       0 |            100 |           99.57 |\n",
      "|       1 |             25 |           26.41 |\n",
      "Total misclassified examples from imbalanced classes (Size of query set):  15\n",
      "#### Selection Complete, Now re-training with augmented subset ####\n",
      "After augmentation, size of train_set:  110  unlabeled set:  1095  val set:  20\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[||||||||||||||||||||]100% [Iteration 5 of 5]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 2 FullTrn,TrainAcc,ValLoss,ValAcc,TstLoss,TstAcc,Time: 0.5441431179642677 0.990909090909091 2.043287694454193 0.35 5.707329511642456 0.6201923076923077 2.287728786468506\n",
      "Gain in accuracy:  8.173076923076927\n",
      "**** Final Metrics after Targeted Learning ****\n",
      "|   Class |   Val Accuracy |   Test Accuracy |\n",
      "|---------+----------------+-----------------|\n",
      "|       0 |            100 |           98.72 |\n",
      "|       1 |             35 |           40    |\n",
      "Gain in overall test accuracy:  8.173076923076927\n",
      "Gain in targeted test accuracy:  6.3700000000000045\n",
      "Total gain in accuracy:  8.173076923076927\n"
     ]
    }
   ],
   "source": [
    "run_targeted_selection(data_name, \n",
    "               datadir, \n",
    "               feature, \n",
    "               model_name, \n",
    "               budget, \n",
    "               split_cfg, \n",
    "               learning_rate, \n",
    "               run, \n",
    "               device, \n",
    "               computeClassErrorLog, \n",
    "               \"SIM\",'fl2mi')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "330ea94d",
   "metadata": {
    "id": "9f6qyrpA8lLc"
   },
   "source": [
    "# GCMI\n",
    "\n",
    "The SMI instantiation of graph-cut (GCMI) is defined as:\n",
    "\\begin{align}\n",
    "I_f(A;Q)=2\\sum_{i \\in A} \\sum_{j \\in Q} sq_{ij}\n",
    "\\end{align}\n",
    "Since maximizing GCMI maximizes the joint pairwise sum with the query set, it will lead to a subset similar to the query set $Q$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "8dc6e91d",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "MM9copHy8lLc",
    "outputId": "1c5e4fe8-2f3c-4137-9f64-c728e00c34d4",
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "pneumoniamnist Custom dataset stats: Train size:  105 Val size:  20 Lake size:  1100\n",
      "Indices of randomly selected classes for imbalance:  [0, 1]\n",
      "Initial training epoch\n",
      "Init model loaded from disk, skipping init training:  ./pneumoniamnist_ResNet18_0.0003_[0, 1]\n",
      "|   Class |   Val Accuracy |   Test Accuracy |\n",
      "|---------+----------------+-----------------|\n",
      "|       0 |            100 |           99.57 |\n",
      "|       1 |             30 |           26.41 |\n",
      "Total misclassified examples from imbalanced classes (Size of query set):  14\n",
      "#### Selection Complete, Now re-training with augmented subset ####\n",
      "After augmentation, size of train_set:  110  unlabeled set:  1095  val set:  20\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[||||||||||||||||||||]100% [Iteration 5 of 5]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 2 FullTrn,TrainAcc,ValLoss,ValAcc,TstLoss,TstAcc,Time: 0.510945912450552 1.0 3.0453184843063354 0.35 7.764312744140625 0.5416666666666666 2.3001534938812256\n",
      "Gain in accuracy:  0.3205128205128176\n",
      "**** Final Metrics after Targeted Learning ****\n",
      "|   Class |   Val Accuracy |   Test Accuracy |\n",
      "|---------+----------------+-----------------|\n",
      "|       0 |            100 |           98.72 |\n",
      "|       1 |             35 |           27.44 |\n",
      "Total gain in accuracy:  0.3205128205128176\n"
     ]
    }
   ],
   "source": [
    "run_targeted_selection(data_name, \n",
    "               datadir, \n",
    "               feature, \n",
    "               model_name, \n",
    "               budget, \n",
    "               split_cfg, \n",
    "               learning_rate, \n",
    "               run, \n",
    "               device, \n",
    "               computeClassErrorLog,\n",
    "               \"SIM\",'gcmi')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "07323a40",
   "metadata": {
    "id": "mGDPAjDR8lLc"
   },
   "source": [
    "# LOGDETMI\n",
    "\n",
    "The SMI instantiation of LogDetMI can be defined as:\n",
    "\\begin{align}\n",
    "I_f(A;Q)=\\log\\det(S_{A}) -\\log\\det(S_{A} - \\eta^2 S_{A,Q}S_{Q}^{-1}S_{A,Q}^T)\n",
    "\\end{align}\n",
    "$S_{A, B}$ denotes the cross-similarity matrix between the items in sets $A$ and $B$. The similarity matrix in constructed in such a way that the cross-similarity between $A$ and $Q$ is multiplied by $\\eta$ to control the trade-off between query-relevance and diversity."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "02ad455b",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "U1QbcrJk8lLd",
    "outputId": "1a7604b8-dee7-468f-d9bf-7e8b202c18d9"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "pneumoniamnist Custom dataset stats: Train size:  105 Val size:  40 Lake size:  1100\n",
      "Indices of randomly selected classes for imbalance:  [0, 1]\n",
      "Initial training epoch\n",
      "Init model loaded from disk, skipping init training:  ./pneumoniamnist_ResNet18_0.0003_[0, 1]\n",
      "|   Class |   Val Accuracy |   Test Accuracy |\n",
      "|---------+----------------+-----------------|\n",
      "|       0 |            100 |           99.57 |\n",
      "|       1 |             25 |           26.41 |\n",
      "Total misclassified examples from imbalanced classes (Size of query set):  15\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[||||||||||||||||||||]100% [Iteration 25 of 25]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "#### Selection Complete, Now re-training with augmented subset ####\n",
      "After augmentation, size of train_set:  130  unlabeled set:  1075  val set:  40\n",
      "Epoch: 2 FullTrn,TrainAcc,ValLoss,ValAcc,TstLoss,TstAcc,Time: 0.6314425319433212 0.9923076923076923 2.1833131536841393 0.725 5.222972393035889 0.6666666666666666 2.7160093784332275\n",
      "Gain in accuracy:  12.82051282051281\n",
      "**** Final Metrics after Targeted Learning ****\n",
      "|   Class |   Val Accuracy |   Test Accuracy |\n",
      "|---------+----------------+-----------------|\n",
      "|       0 |            100 |           98.72 |\n",
      "|       1 |             45 |           47.44 |\n",
      "Total gain in accuracy:  12.82051282051281\n"
     ]
    }
   ],
   "source": [
    "run_targeted_selection(data_name, \n",
    "               datadir, \n",
    "               feature, \n",
    "               model_name, \n",
    "               budget, \n",
    "               split_cfg, \n",
    "               learning_rate, \n",
    "               run, \n",
    "               device, \n",
    "               computeClassErrorLog,\n",
    "               \"SIM\",'logdetmi')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b1ec7d2d",
   "metadata": {
    "id": "3dZqCZ1v8lLe"
   },
   "source": [
    "# Random"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "9389578a",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "FlbOpp438lLe",
    "outputId": "f754727a-dfb5-4853-d4c1-8a96115182db",
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "pneumoniamnist Custom dataset stats: Train size:  105 Val size:  40 Lake size:  1100\n",
      "Indices of randomly selected classes for imbalance:  [0, 1]\n",
      "Initial training epoch\n",
      "Init model loaded from disk, skipping init training:  ./pneumoniamnist_ResNet18_0.0003_[0, 1]\n",
      "|   Class |   Val Accuracy |   Test Accuracy |\n",
      "|---------+----------------+-----------------|\n",
      "|       0 |            100 |           99.57 |\n",
      "|       1 |             25 |           26.41 |\n",
      "#### Selection Complete, Now re-training with augmented subset ####\n",
      "After augmentation, size of train_set:  130  unlabeled set:  1075  val set:  40\n",
      "Epoch: 2 FullTrn,TrainAcc,ValLoss,ValAcc,TstLoss,TstAcc,Time: 0.5391155630350113 0.9923076923076923 3.127401575446129 0.575 7.029404997825623 0.5288461538461539 2.6538174152374268\n",
      "Gain in accuracy:  -0.9615384615384599\n",
      "**** Final Metrics after Targeted Learning ****\n",
      "|   Class |   Val Accuracy |   Test Accuracy |\n",
      "|---------+----------------+-----------------|\n",
      "|       0 |            100 |           99.57 |\n",
      "|       1 |             15 |           24.87 |\n",
      "Total gain in accuracy:  -0.9615384615384599\n"
     ]
    }
   ],
   "source": [
    "run_targeted_selection(data_name, \n",
    "               datadir, \n",
    "               feature, \n",
    "               model_name, \n",
    "               budget, \n",
    "               split_cfg, \n",
    "               learning_rate, \n",
    "               run, \n",
    "               device, \n",
    "               computeClassErrorLog,\n",
    "               \"random\",'random')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d70fddb5",
   "metadata": {},
   "source": [
    "# US"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6164c9b4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "pneumoniamnist Custom dataset stats: Train size:  105 Val size:  40 Lake size:  1100\n",
      "Indices of randomly selected classes for imbalance:  [0, 1]\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'EntropySampling' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[39], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m run_targeted_selection(data_name, \n\u001b[1;32m      2\u001b[0m                datadir, \n\u001b[1;32m      3\u001b[0m                feature, \n\u001b[1;32m      4\u001b[0m                model_name, \n\u001b[1;32m      5\u001b[0m                budget, \n\u001b[1;32m      6\u001b[0m                split_cfg, \n\u001b[1;32m      7\u001b[0m                learning_rate, \n\u001b[1;32m      8\u001b[0m                run, \n\u001b[1;32m      9\u001b[0m                device, \n\u001b[1;32m     10\u001b[0m                computeClassErrorLog,\n\u001b[1;32m     11\u001b[0m                \u001b[39m\"\u001b[39;49m\u001b[39mAL\u001b[39;49m\u001b[39m\"\u001b[39;49m,\u001b[39m'\u001b[39;49m\u001b[39mus\u001b[39;49m\u001b[39m'\u001b[39;49m)\n",
      "Cell \u001b[0;32mIn[32], line 75\u001b[0m, in \u001b[0;36mrun_targeted_selection\u001b[0;34m(dataset_name, datadir, feature, model_name, budget, split_cfg, learning_rate, run, device, computeErrorLog, strategy, sf)\u001b[0m\n\u001b[1;32m     73\u001b[0m     strategy_sel \u001b[39m=\u001b[39m BADGE(train_set, unlabeled_lake_set, model, num_cls, strategy_args)\n\u001b[1;32m     74\u001b[0m \u001b[39melif\u001b[39;00m(sf\u001b[39m==\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mus\u001b[39m\u001b[39m\"\u001b[39m):\n\u001b[0;32m---> 75\u001b[0m     strategy_sel \u001b[39m=\u001b[39m EntropySampling(train_set, unlabeled_lake_set, model, num_cls, strategy_args)\n\u001b[1;32m     76\u001b[0m \u001b[39melif\u001b[39;00m(sf\u001b[39m==\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mglister\u001b[39m\u001b[39m\"\u001b[39m \u001b[39mor\u001b[39;00m sf\u001b[39m==\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mglister-tss\u001b[39m\u001b[39m\"\u001b[39m):\n\u001b[1;32m     77\u001b[0m     strategy_sel \u001b[39m=\u001b[39m GLISTER(train_set, unlabeled_lake_set, model, num_cls, strategy_args, val_set, typeOf\u001b[39m=\u001b[39m\u001b[39m'\u001b[39m\u001b[39mrand\u001b[39m\u001b[39m'\u001b[39m, lam\u001b[39m=\u001b[39m\u001b[39m0.1\u001b[39m)\n",
      "\u001b[0;31mNameError\u001b[0m: name 'EntropySampling' is not defined"
     ]
    }
   ],
   "source": [
    "run_targeted_selection(data_name, \n",
    "               datadir, \n",
    "               feature, \n",
    "               model_name, \n",
    "               budget, \n",
    "               split_cfg, \n",
    "               learning_rate, \n",
    "               run, \n",
    "               device, \n",
    "               computeClassErrorLog,\n",
    "               \"AL\",'us')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ebc215ed",
   "metadata": {},
   "source": [
    "# BADGE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "61e524cd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "pneumoniamnist Custom dataset stats: Train size:  105 Val size:  40 Lake size:  1103\n",
      "Indices of randomly selected classes for imbalance:  [0, 1]\n",
      "Initial training epoch\n",
      "Init model loaded from disk, skipping init training:  ./pneumoniamnist_ResNet18_0.0003_1\n",
      "|   Class |   Val Accuracy |   Test Accuracy |\n",
      "|---------+----------------+-----------------|\n",
      "|       0 |            100 |           99.57 |\n",
      "|       1 |             15 |            7.95 |\n",
      "#### Selection Complete, Now re-training with augmented subset ####\n",
      "After augmentation, size of train_set:  125  unlabeled set:  1083  val set:  40\n",
      "Epoch: 2 FullTrn,TrainAcc,ValLoss,ValAcc,TstLoss,TstAcc,Time: 0.3409885913133621 0.992 2.3366213962435722 0.725 6.24738883972168 0.5961538461538461 4.516126394271851\n",
      "Gain in accuracy:  17.307692307692307\n",
      "**** Final Metrics after Targeted Learning ****\n",
      "|   Class |   Val Accuracy |   Test Accuracy |\n",
      "|---------+----------------+-----------------|\n",
      "|       0 |            100 |           98.72 |\n",
      "|       1 |             45 |           36.15 |\n",
      "Total gain in accuracy:  17.307692307692307\n"
     ]
    }
   ],
   "source": [
    "run_targeted_selection(data_name, \n",
    "               datadir, \n",
    "               feature, \n",
    "               model_name, \n",
    "               budget, \n",
    "               split_cfg, \n",
    "               learning_rate, \n",
    "               run, \n",
    "               device, \n",
    "               computeClassErrorLog,\n",
    "               \"AL\",'badge')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4ecf8b9c",
   "metadata": {},
   "source": [
    "# References\n",
    "[1] Rishabh Iyer, Ninad Khargoankar, Jeff Bilmes, and Himanshu Asnani. Submodular combinatorialinformation measures with applications in machine learning.arXiv preprint arXiv:2006.15412,2020\n",
    "\n",
    "\n",
    "[2] Kaushal V, Kothawade S, Ramakrishnan G, Bilmes J, Iyer R. PRISM: A Unified Framework of Parameterized Submodular Information Measures for Targeted Data Subset Selection and Summarization. arXiv preprint arXiv:2103.00128. 2021 Feb 27.\n",
    "\n",
    "\n",
    "[3] Anupam Gupta and Roie Levin. The online submodular cover problem. InACM-SIAM Symposiumon Discrete Algorithms, 2020"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e93a22fd",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "machine_shape": "hm",
   "name": "submodlib_cifar_classimb.ipynb",
   "provenance": [],
   "toc_visible": true
  },
  "kernelspec": {
   "display_name": "trust",
   "language": "python",
   "name": "trust"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.5"
  },
  "vscode": {
   "interpreter": {
    "hash": "f2040589891662758f5d1e69c3acf00609bacba1ba894859d6203b522c532974"
   }
  },
  "widgets": {
   "application/vnd.jupyter.widget-state+json": {
    "0074c4d4c28d4dad87a3eaaac8458479": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "14a33535616d4fa993d20e525666af04": {
     "model_module": "@jupyter-widgets/base",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "4ee6a0573c6845458996a59679ac6068": {
     "model_module": "@jupyter-widgets/base",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "6792820b58df4d2b86386c29aaad1a9f": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_cbe081bb90d04faaae98fef1c9921180",
      "placeholder": "‚Äã",
      "style": "IPY_MODEL_0074c4d4c28d4dad87a3eaaac8458479",
      "value": " 170499072/? [00:07&lt;00:00, 23215900.76it/s]"
     }
    },
    "9d386f74f6c4469bbbf1ac6b2a49afed": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "ProgressStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "ProgressStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "bar_color": null,
      "description_width": "initial"
     }
    },
    "bb6c0909711948b98f0a0b767cb9201e": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "FloatProgressModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "FloatProgressModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "ProgressView",
      "bar_style": "success",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_4ee6a0573c6845458996a59679ac6068",
      "max": 170498071,
      "min": 0,
      "orientation": "horizontal",
      "style": "IPY_MODEL_9d386f74f6c4469bbbf1ac6b2a49afed",
      "value": 170498071
     }
    },
    "cbe081bb90d04faaae98fef1c9921180": {
     "model_module": "@jupyter-widgets/base",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "d978b42c8c2d4ce69ca7a3233094ba73": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "HBoxModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HBoxModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HBoxView",
      "box_style": "",
      "children": [
       "IPY_MODEL_bb6c0909711948b98f0a0b767cb9201e",
       "IPY_MODEL_6792820b58df4d2b86386c29aaad1a9f"
      ],
      "layout": "IPY_MODEL_14a33535616d4fa993d20e525666af04"
     }
    }
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
